{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72fae4e-f7de-42b7-91ee-bdd0a57ae46c",
   "metadata": {},
   "source": [
    "# Prompt Generation from User Requirements\n",
    "\n",
    "https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/chatbots/information-gather-prompting.ipynb\n",
    "\n",
    "A **chat bot** that helps a user **generate a prompt**.\n",
    "It will first collect requirements from the user, and then will generate the prompt (and refine it based on user input).\n",
    "These are split into two separate states, and the LLM decides when to transition between them.\n",
    "\n",
    "![information-gather-prompting](information-gather-prompting.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb66b808",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install our required packages and set our OpenAI API key (the LLM we will use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa583d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "% pip install -U langgraph langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78b593-ba26-4c90-b2e2-83119e47679f",
   "metadata": {},
   "source": [
    "## Gather information\n",
    "\n",
    "First, let's define the part of the graph that will gather user requirements. This will be an LLM call with a specific system message. It will have access to a tool that it can call when it is ready to generate the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d731dcc-8295-498d-a95f-644ce24a717e",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\">\n",
    "    <p class=\"admonition-title\">Using Pydantic with LangChain</p>\n",
    "    <p>\n",
    "        This notebook uses Pydantic v2 <code>BaseModel</code>, which requires <code>langchain-core >= 0.3</code>. Using <code>langchain-core < 0.3</code> will result in errors due to mixing of Pydantic v1 and v2 <code>BaseModels</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53216ab5-2cd3-48a4-8778-41ba10f72519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import os\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f795b78-004d-40ca-95d6-069f67e4f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Your job is to get information from a user about what type of prompt template they want to create.\n",
    "\n",
    "You should get the following information from them:\n",
    "\n",
    "- What the objective of the prompt is\n",
    "- What variables will be passed into the prompt template\n",
    "- Any constraints for what the output should NOT do\n",
    "- Any requirements that the output MUST adhere to\n",
    "\n",
    "If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "\n",
    "After you are able to discern all the information, call the relevant tool.\"\"\"\n",
    "\n",
    "\n",
    "def get_messages_info(messages):\n",
    "    return [SystemMessage(content=template)] + messages\n",
    "    \n",
    "# Where is Berlin?\n",
    "#   Args:\n",
    "#     constraints: [\"city\"]\n",
    "#     objective: locate\n",
    "#     requirements: [\"geographic location\"]\n",
    "#     variables: [\"Berlin\"]\n",
    "class PromptInstructions(BaseModel):\n",
    "    \"\"\"Instructions on how to prompt the LLM.\"\"\"\n",
    "\n",
    "    objective: str\n",
    "    variables: List[str]\n",
    "    constraints: List[str]\n",
    "    requirements: List[str]\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"NA\"\n",
    "llm = ChatOpenAI(\n",
    "    model = \"llama3.2\",\n",
    "    base_url = \"http://localhost:11434/v1\")\n",
    "\n",
    "llm_with_tool = llm.bind_tools([PromptInstructions])\n",
    "\n",
    "\n",
    "def info_chain(state):\n",
    "    messages = get_messages_info(state[\"messages\"])\n",
    "    print(\"Gather Information input:\", messages)\n",
    "    response = llm_with_tool.invoke(messages)\n",
    "    print(\"Gather Information output:\", response)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb40630f-83c7-4283-a6dd-04231805a7ed",
   "metadata": {},
   "source": [
    "## Generate Prompt\n",
    "\n",
    "We now set up the state that will generate the prompt.\n",
    "This will require a separate system message, as well as a function to filter out all message PRIOR to the tool invocation (as that is when the previous state decided it was time to generate the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca9a0234-bbeb-4bff-8276-8dde499c3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "\n",
    "# New system prompt\n",
    "prompt_system = \"\"\"Based on the following requirements, write a good prompt template:\n",
    "\n",
    "{reqs}\"\"\"\n",
    "\n",
    "\n",
    "# Function to get the messages for the prompt\n",
    "# Will only get messages AFTER the tool call\n",
    "def get_prompt_messages(messages: list):\n",
    "    tool_call = None\n",
    "    other_msgs = []\n",
    "    for m in messages:\n",
    "        if isinstance(m, AIMessage) and m.tool_calls:\n",
    "            tool_call = m.tool_calls[0][\"args\"]\n",
    "        elif isinstance(m, ToolMessage):\n",
    "            continue\n",
    "        elif tool_call is not None:\n",
    "            other_msgs.append(m)\n",
    "    print(\"tool_call:\", tool_call)\n",
    "    return [SystemMessage(content=prompt_system.format(reqs=tool_call))] + other_msgs\n",
    "\n",
    "\n",
    "def prompt_gen_chain(state):\n",
    "    messages = get_prompt_messages(state[\"messages\"])\n",
    "    print(\"Prompt Generation input:\", messages)\n",
    "    response = llm.invoke(messages)\n",
    "    print(\"Prompt Generation output:\", response)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbabda8-34f0-4eef-bce2-ad3ff505366b",
   "metadata": {},
   "source": [
    "## Define the state logic\n",
    "\n",
    "This is the logic for what state the chatbot is in.\n",
    "If the last message is a tool call, then we are in the state where the \"prompt creator\" (`prompt`) should respond.\n",
    "Otherwise, if the last message is not a HumanMessage, then we know the human should respond next and so we are in the `END` state.\n",
    "If the last message is a HumanMessage, then if there was a tool call previously we are in the `prompt` state.\n",
    "Otherwise, we are in the \"info gathering\" (`info`) state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74f29e15-20e2-420c-a450-84e929f16e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "def get_state(state):\n",
    "    messages = state[\"messages\"]\n",
    "    message=messages[-1]\n",
    "    print(\"get_state message:\", message)\n",
    "    if isinstance(message, AIMessage) and message.tool_calls:\n",
    "        print(\"get_state Goto addToolMessage\")\n",
    "        return \"addToolMessage\"\n",
    "    elif not isinstance(message, HumanMessage):\n",
    "        print(\"get_state Goto END\")\n",
    "        return END\n",
    "    return \"GatherInformation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76bea78-07a5-418f-9b7c-71c376d4b6f7",
   "metadata": {},
   "source": [
    "## Create the graph\n",
    "\n",
    "We can now the create the graph.\n",
    "We will use a SqliteSaver to persist conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59d9d6b4-dce4-43cc-9a1a-61a7912ed5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"GatherInformation\", info_chain)\n",
    "workflow.add_node(\"GeneratePrompt\", prompt_gen_chain)\n",
    "\n",
    "\n",
    "@workflow.add_node\n",
    "def addToolMessage(state: State):\n",
    "    message=state[\"messages\"][-1]\n",
    "    print(\"addToolMessage input:\", message)\n",
    "    tool_call_id = message.tool_calls[0][\"id\"]\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=\"Prompt generated!\",\n",
    "                tool_call_id=tool_call_id,\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\"GatherInformation\", get_state, [\"addToolMessage\", \"GatherInformation\", END])\n",
    "workflow.add_edge(\"addToolMessage\", \"GeneratePrompt\")\n",
    "workflow.add_edge(\"GeneratePrompt\", END)\n",
    "workflow.add_edge(START, \"GatherInformation\")\n",
    "graph = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1613e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf523c-265d-45cf-a981-fc50c50c1738",
   "metadata": {},
   "source": [
    "## Use the graph\n",
    "\n",
    "We can now use the created chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25793988-45a2-4e65-b33c-64e72aadb10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "cached_human_responses = [\"hi!\", \"rag prompt\", \"1 rag, 2 none, 3 no, 4 no\", \"red\", \"q\"]\n",
    "cached_response_index = 0\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "while True:\n",
    "    try:\n",
    "        user = input(\"User (q/Q to quit): \")\n",
    "    except:\n",
    "        user = cached_human_responses[cached_response_index]\n",
    "        cached_response_index += 1\n",
    "    print(f\"User (q/Q to quit): {user}\")\n",
    "    if user in {\"q\", \"Q\"}:\n",
    "        print(\"AI: Byebye\")\n",
    "        break\n",
    "    output = None\n",
    "    for output in graph.stream(\n",
    "        {\"messages\": [HumanMessage(content=user)]}, config=config, stream_mode=\"updates\"\n",
    "    ):\n",
    "        last_message = next(iter(output.values()))[\"messages\"][-1]\n",
    "        last_message.pretty_print()\n",
    "\n",
    "    if output and \"prompt\" in output:\n",
    "        print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
