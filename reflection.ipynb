{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "492f050f-3dc3-44fa-8fdc-03362afd5488",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/reflection/reflection.ipynb\n",
    "\n",
    "In the context of LLM agent building, reflection refers to **the process of prompting an LLM to observe its past steps (along with potential observations from tools/the environment) to assess the quality of the chosen actions**.\n",
    "This is then used downstream for things like re-planning, search, or evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef94e7e-c9a5-4eee-a865-acf411b5c235",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install our required packages and set our API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3368f330-cad6-4d35-a291-68fbf4389d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27bcc4a-aaa5-46bd-8163-3e0e90cb66e6",
   "metadata": {},
   "source": [
    "## Generate\n",
    "\n",
    "For our example, we will create a \"5 paragraph essay\" generator. First, create the generator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc10028f-9cef-4936-9419-cbdf06d24f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an essay assistant tasked with writing excellent 5-paragraph essays.\"\n",
    "            \" Generate the best essay possible for the user's request.\"\n",
    "            \" If the user provides critique, respond with a revised version of your previous attempts.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama3.2\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    ")\n",
    "generate = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbe25dc-fd1e-4ed5-a3c8-fed830b46d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay = \"\"\n",
    "request = HumanMessage(\n",
    "    content=\"Write an essay on why the little prince is relevant in modern childhood\"\n",
    ")\n",
    "for chunk in generate.stream({\"messages\": [request]}):\n",
    "    print(chunk.content, end=\"\")\n",
    "    essay += chunk.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b276e7-c392-4eec-be75-c77bd130379d",
   "metadata": {},
   "source": [
    "### Reflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a705be92-88c0-4f4f-b4c2-cdcd9af8cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a teacher grading an essay submission. Generate critique and recommendations for the user's submission.\"\n",
    "            \" Provide detailed recommendations, including requests for length, depth, style, etc.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "reflect = reflection_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c5eb2a-8bce-48ab-b87d-9dacb9b64ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection = \"\"\n",
    "for chunk in reflect.stream({\"messages\": [request, HumanMessage(content=essay)]}):\n",
    "    print(chunk.content, end=\"\")\n",
    "    reflection += chunk.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf926c-1174-4e96-91b9-57c57cfce40d",
   "metadata": {},
   "source": [
    "### Repeat\n",
    "\n",
    "And... that's all there is too it! You can repeat in a loop for a fixed number of steps, or use an LLM (or other check) to decide when the finished product is good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf99a8-3aa0-4e09-936e-8452c35fa84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in generate.stream(\n",
    "    {\"messages\": [request, AIMessage(content=essay), HumanMessage(content=reflection)]}\n",
    "):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63a9d93-a14d-4e41-a4bb-a4cd31713f44",
   "metadata": {},
   "source": [
    "## Define graph\n",
    "\n",
    "Now that we've shown each step in isolation, we can wire it up in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a9d7c-5d2e-4194-b745-4511ec20db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Sequence\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "async def generation_node(state: State) -> State:\n",
    "    return {\"messages\": [await generate.ainvoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "async def reflection_node(state: State) -> State:\n",
    "    # Other messages we need to adjust\n",
    "    cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}\n",
    "    # First message is the original user request. We hold it the same for all nodes\n",
    "    translated = [state[\"messages\"][0]] + [\n",
    "        cls_map[msg.type](content=msg.content) for msg in state[\"messages\"][1:]\n",
    "    ]\n",
    "    res = await reflect.ainvoke(translated)\n",
    "    # We treat the output of this as human feedback for the generator\n",
    "    return {\"messages\": [HumanMessage(content=res.content)]}\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_edge(START, \"generate\")\n",
    "\n",
    "\n",
    "def should_continue(state: State):\n",
    "    if len(state[\"messages\"]) > 6:\n",
    "        # End after 3 iterations\n",
    "        return END\n",
    "    return \"reflect\"\n",
    "\n",
    "\n",
    "builder.add_conditional_edges(\"generate\", should_continue)\n",
    "builder.add_edge(\"reflect\", \"generate\")\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ce60a-8b7d-4258-99d1-52705146844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06263a07-8a15-4ec3-b692-1c6cef3b1c1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "async for event in graph.astream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Generate an essay on the topicality of The Little Prince and its message in modern life\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    config,\n",
    "):\n",
    "    print(event)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced83251-8edc-483d-a03f-5bd884ea8d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394bf0df-fc28-4104-a278-a56c9cb8b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatPromptTemplate.from_messages(state.values[\"messages\"]).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa62df2-e8ee-40dd-ac95-9d982eae6079",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Now that you've applied reflection to an LLM agent, I'll note one thing: self-reflection is inherently cyclic: it is much more effective if the reflection step has additional context or feedback (from tool observations, checks, etc.). If, like in the scenario above, the reflection step simply prompts the LLM to reflect on its output, it can still benefit the output quality (since the LLM then has multiple \"shots\" at getting a good output), but it's less guaranteed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lg_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
