{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "425fb020-e864-40ce-a31f-8da40c73d14b",
   "metadata": {},
   "source": [
    "# Agentic RAG\n",
    "\n",
    "https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/rag/langgraph_agentic_rag.ipynb\n",
    "\n",
    "[Retrieval Agents](https://python.langchain.com/docs/tutorials/qa_chat_history/#agents) are useful when we want to make decisions about whether to retrieve from an index.\n",
    "\n",
    "To implement a retrieval agent, we simply need to give an LLM access to a retriever tool.\n",
    "\n",
    "We can incorporate this into [LangGraph](https://langchain-ai.github.io/langgraph/).\n",
    "\n",
    "![agentic_rag](agentic_rag.png)\n",
    "\n",
    "\n",
    "> **Run on WSL, windows cannot support well or embeddings!**\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's download the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969fb438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture --no-stderr\n",
    "# %pip install -U --quiet langchain-community tiktoken langchain-openai langchainhub chromadb langchain langgraph langchain-text-splitters\n",
    "# %pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74e4532",
   "metadata": {},
   "source": [
    "## Retriever\n",
    "\n",
    "First, we index 3 blog posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50c9efe-4abe-42fa-b35a-05eeeede9ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"USER_AGENT\"] = \"LangChain/RAG-Application\"\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import MHTMLLoader\n",
    "from pathlib import Path\n",
    "\n",
    "# Load documents from local MHTML files\n",
    "# urls = [\n",
    "#     \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "#     \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "#     \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "# ]\n",
    "files = [\n",
    "    \"data/LLM Powered Autonomous Agents.mhtml\",\n",
    "    \"data/Prompt Engineering.mhtml\",\n",
    "    \"data/Adversarial Attacks on LLMs.mhtml\",\n",
    "]\n",
    "docs = [MHTMLLoader(Path(file)).load() for file in files]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Embeddings model\n",
    "# https://ollama.com/blog/embedding-models\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"mxbai-embed-large\"\n",
    ")\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"agentic-rag-chroma\",\n",
    "    persist_directory=\".\",\n",
    "    embedding=embeddings,    \n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "# documents = retriever.invoke(\"What does Lilian Weng say about the types of agent memory?\")\n",
    "# documents[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d2277-45b2-4ae8-a7d6-62b07fb4a002",
   "metadata": {},
   "source": [
    "Then we create a retriever tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b97bdd8-d7e3-444d-ac96-5ef4725f9048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e8f78-1ef7-42ad-b2bf-835ed5850553",
   "metadata": {},
   "source": [
    "## Agent State\n",
    " \n",
    "We will define a graph.\n",
    "\n",
    "A `state` object that it passes around to each node.\n",
    "\n",
    "Our state will be a list of `messages`.\n",
    "\n",
    "Each node in our graph will append to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e378706-47d5-425a-8ba0-57b9acffbd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc949d42-8a34-4231-bff0-b8198975e2ce",
   "metadata": {},
   "source": [
    "## Nodes and Edges\n",
    "\n",
    "We can lay out an agentic RAG graph like this:\n",
    "\n",
    "* The state is a set of messages\n",
    "* Each node will update (append to) state\n",
    "* Conditional edges decide which node to visit next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7a7843-2a4a-4de5-a211-ff2efe3aba47",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\">\n",
    "    <p class=\"admonition-title\">Using Pydantic with LangChain</p>\n",
    "    <p>\n",
    "        This notebook uses Pydantic v2 <code>BaseModel</code>, which requires <code>langchain-core >= 0.3</code>. Using <code>langchain-core < 0.3</code> will result in errors due to mixing of Pydantic v1 and v2 <code>BaseModels</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d1d83-dda6-4de4-bf8b-be9965c227fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain import hub\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"NA\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"NA\"\n",
    "### Edges\n",
    "\n",
    "\n",
    "def GradeDocumentsEdges(state) -> Literal[\"Generater\", \"ReWriter\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(\n",
    "        model = \"llama3.2\",\n",
    "        base_url = \"http://localhost:11434/v1\", \n",
    "        streaming=True\n",
    "    )\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"Generater\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"ReWriter\"\n",
    "\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "def retrieveAgent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]    \n",
    "    model = ChatOpenAI(\n",
    "        model = \"llama3.2\",\n",
    "        base_url = \"http://localhost:11434/v1\", \n",
    "        streaming=True\n",
    "    )\n",
    "    model = model.bind_tools(tools=[retriever_tool])\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatOpenAI(\n",
    "        model = \"llama3.2\",\n",
    "        base_url = \"http://localhost:11434/v1\", \n",
    "        streaming=True\n",
    "    )\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(\n",
    "        model = \"llama3.2\",\n",
    "        base_url = \"http://localhost:11434/v1\", \n",
    "        streaming=True\n",
    "    )\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955882ef-7467-48db-ae51-de441f2fc3a7",
   "metadata": {},
   "source": [
    "## Graph\n",
    "\n",
    "* Start with an agent, `call_model`\n",
    "* Agent make a decision to call a function\n",
    "* If so, then `action` to call tool (retriever)\n",
    "* Then call agent with the tool output added to messages (`state`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8718a37f-83c2-4f16-9850-e61e0f49c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"RetrieveAgent\", retrieveAgent)  # agent\n",
    "workflow.add_node(\"GradeDocuments\", ToolNode(tools=[retriever_tool]))  # retrieval\n",
    "workflow.add_node(\"ReWriter\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\"Generater\", generate)  # Generating a response after we know the documents are relevant\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"RetrieveAgent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    source=\"RetrieveAgent\",\n",
    "    # Assess agent decision\n",
    "    path=tools_condition,\n",
    "    path_map={\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"GradeDocuments\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"GradeDocuments\",\n",
    "    # Assess agent decision\n",
    "    GradeDocumentsEdges,\n",
    ")\n",
    "workflow.add_edge(\"Generater\", END)\n",
    "workflow.add_edge(\"ReWriter\", \"RetrieveAgent\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7649f05a-cb67-490d-b24a-74d41895139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"What does Lilian Weng say about the types of agent memory?\"),\n",
    "    ]\n",
    "}\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lg_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
